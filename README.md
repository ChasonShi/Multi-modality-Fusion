# Multi-modality-Fusion
A Multi-modality Fusion Paper Reading List

### Survey

- [Multimodal Learning with Transformers : A Survey](https://arxiv.org/pdf/2206.06488.pdf).  [Peng Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+P), [Xiatian Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+X), [David A. Clifton](https://arxiv.org/search/cs?searchtype=author&query=Clifton%2C+D+A).  ***ArXiv 2022***. **Summary : ** Survey transformer based methods for MML
- [Multimodal Intelligence: Representation Learning,Information Fusion, and Applications](https://arxiv.org/pdf/1911.03977.pdf). [Chao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+C), [Zichao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Z), [Xiaodong He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+X), [Li Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+L).  ***ArXiv 2019***.  **Summary : ** Survey deep learning methods for MML
- [Multimodal Machine Learning:A Survey and Taxonomy](https://arxiv.org/pdf/1705.09406.pdf).Tadas BaltruË‡saitis, Chaitanya Ahuja, and Louis-Philippe Morency. ***ArXiv 2017***. **Summary : ** A classical survey of MML, given five challenges of MML.



### VLP (Visual Language Pretrain)

* [Multi-modal Alignment using Representation Codebook](https://arxiv.org/pdf/2203.00048.pdf).  Jiali Duan, Liqun Chen, Son Tran, Jinyu Yang, Yi Xu, Belinda Zeng, Trishul Chilimbi. ***CVPR 2022***.
* [Vision-Language Pre-Training with Triple Contrastive Learning](https://arxiv.org/pdf/2202.10401.pdf). Jinyu Yang, Jiali Duan, Son Tran, Yi Xu , Sampath Chanda, Liqun Chen , Belinda Zeng , Trishul Chilimbi , and Junzhou Huang. ***CVPR 2022***. **Summary : ** Proposed three contrastive learning objectives for VLP,  for intra-modal and inter-modal  modeling.                 **Contribution:**  Further considers intra-modal supervision in turn benefits cross-modal.
* [An Empirical Study of Training End-to-End Vision-and-Language Transformers](https://arxiv.org/pdf/2111.02387.pdf). [Zi-Yi Dou](https://arxiv.org/search/cs?searchtype=author&query=Dou%2C+Z), [Yichong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z),[Jianfeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Shuohang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Chenguang Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu%2C+C), [Pengchuan Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), [Lu Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+L), [Nanyun Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng%2C+N), [Zicheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Michael Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng%2C+M) ***CVPR 2022***   **Summary : ** discuss the model designs along multiple dimensions: visual encoder, text encoder, model architecture, pre-training objectives and find a best design       **Contribution:**   present a VLP METER, 
* [VL-BEIT: Generative Vision-Language Pretraining](https://arxiv.org/pdf/2206.01127.pdf).  Hangbo Bao , Wenhui Wang , Li Dong, Furu Wei. ***ArXiv 2022***.
* [Align before Fuse: Vision and Language Representation Learning with Momentum Distillation](https://arxiv.org/pdf/2107.07651.pdf). *Junnan Li, Ramprasaath R. Selvaraju, Akhilesh D. Gotmare Shafiq Joty, Caiming Xiong, Steven C.H. Hoi*.***NIPS 2021***.
* [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/pdf/2102.03334.pdf). [Wonjae Kim](https://arxiv.org/search/stat?searchtype=author&query=Kim%2C+W), [Bokyung Son](https://arxiv.org/search/stat?searchtype=author&query=Son%2C+B), [Ildoo Kim](https://arxiv.org/search/stat?searchtype=author&query=Kim%2C+I). ***ICLM 2021***.
* [ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/pdf/1908.02265.pdf). *Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee*. ***NIPS 2019***.
* [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557.pdf). *[Liunian Harold Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L+H), [Mark Yatskar](https://arxiv.org/search/cs?searchtype=author&query=Yatskar%2C+M), [Da Yin](https://arxiv.org/search/cs?searchtype=author&query=Yin%2C+D), [Cho-Jui Hsieh](https://arxiv.org/search/cs?searchtype=author&query=Hsieh%2C+C), [Kai-Wei Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+K)*. ***ArXiv  2019***.
* [UNITER: UNiversal Image-TExt Representation Learning](https://arxiv.org/pdf/1909.11740.pdf). *[Yen-Chun Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+Y), [Linjie Li](https://arxiv.org/search/cs?searchtype=author&query=Li%2C+L), [Licheng Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+L), [Ahmed El Kholy](https://arxiv.org/search/cs?searchtype=author&query=Kholy%2C+A+E), [Faisal Ahmed](https://arxiv.org/search/cs?searchtype=author&query=Ahmed%2C+F), [Zhe Gan](https://arxiv.org/search/cs?searchtype=author&query=Gan%2C+Z), [Yu Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+Y), [Jingjing Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+J)*. ***ECCV  2019***.



### Video & Audio

* [Attention Bottlenecks for Multimodal Fusion](https://arxiv.org/pdf/2107.00135.pdf). *[Arsha Nagrani](https://arxiv.org/search/cs?searchtype=author&query=Nagrani%2C+A), [Shan Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+S), [Anurag Arnab](https://arxiv.org/search/cs?searchtype=author&query=Arnab%2C+A), [Aren Jansen](https://arxiv.org/search/cs?searchtype=author&query=Jansen%2C+A), [Cordelia Schmid](https://arxiv.org/search/cs?searchtype=author&query=Schmid%2C+C), [Chen Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+C)* .***NIPS 2021***.

* [ViViT: A Video Vision Transformer](https://arxiv.org/pdf/2103.15691.pdf). *Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, Cordelia Schmid*.***ICCV 2021***.
* [ Multiview Transformers for Video Recognition](https://arxiv.org/pdf/2201.04288.pdf). *[Shen Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan%2C+S), [Xuehan Xiong](https://arxiv.org/search/cs?searchtype=author&query=Xiong%2C+X), [Anurag Arnab](https://arxiv.org/search/cs?searchtype=author&query=Arnab%2C+A), [Zhichao Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu%2C+Z), [Mi Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+M), [Chen Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+C), [Cordelia Schmid](https://arxiv.org/search/cs?searchtype=author&query=Schmid%2C+C)*.***CVPR 2022***.



### Video & Text

* [ VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/pdf/1904.01766.pdf). *Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid*. ***ICCV 2019***.
